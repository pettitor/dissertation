\section{Background and System Description}\label{sec:hierarchical:related_work}

In this section we introduce content popularity and traffic models used to evaluate the performance of contents delivery networks.
We describe the systems model for hierarchical caching systems.
We present a representative set of cache eviction policies and provide an overview of recent efforts in modeling the performance of isolated and interconnected caches.

\subsection{System Model for Hierarchical Caching Systems}\label{sec:hierarchical:system_model}

%The vast expansion of streaming services and content delivery networks (CDNs) suggests addressing content by content centric networking (CCN), to enable effective traffic management.
%Addressing content instead of physical servers allows utilizing local resources.
%To enable addressing, content is identified with unique names.
%Available storage in routers is used to cache content in these approaches.
Modern content delivery networks use a tree-like structure of content caches to deliver content efficiently.
Since the tree-like structure of caches spreads on the way to the end user, the content replication scales with the content demand.

\begin{figure}[tb]
\centering
\includegraphics[width=0.9\textwidth]{hierarchical/analyticbw/figures/hcmodeln1}
\caption{System model.}
\label{fig:hcmodel}
\end{figure}

To model tree-like content delivery networks, we consider a set of caches $\Gamma$ that is organized in a tiered caching architecture as depicted in \reffig{fig:hcmodel}. Each cache in $\Gamma$ has a certain cache capacity $C$, which specifies the number of content items it can store.
%TODO rephrase
%bin-packing problems can arise and therefore size has been considered as an important factor in studies which assume complete files as transport unit.

If complete files are considered as transport unit, file size can have an impact due to bin-backing problems.
However, content delivery networks and coding schemes for video streams are segmenting data into small chunks in the kB range.
Therefore, we simply assume objects of fixed size corresponding to data chunks.
Consider that the methods can also be applied to content with varying file size, if the sums are weighted accordingly.

Tier-1 caches have capacity $C_{1_i}, i\in\{1,...,n_1\}$ and tier-2 caches have capacity $C_{2_i}, i\in\{1,...,n_2\}$.
%We use the convention $C_1=C_{1_i}, \forall i$ if we assume the same cache size for all tier-1 caches.
Here we assume that the cache capacity of all tier-1 caches is equal and use the convention $C_1=C_{1_i}, \forall i$.
This is for example the case if the caches are deployed by a provider on customer premise equipment.
If the caches are set up by end-users the cache capacities may vary.
Each tier-1 cache $i$ has a specific average upload throughput $\rho_{1_i}$.
Content items are requested from a catalog with size $N$.
Each item $m\in \{1,2,\dots,N\}$ is requested with rate $\lambda_m$.
The total arrival rate of requests is $\lambda=\sum_{m=1}^N \lambda_m$.
%\begin{equation}
%\lambda=\sum_{m=1}^N \lambda_m \, .
%\end{equation}

The arrival rate of requests for an item can then also be expressed with the probability $p_m$ that item $m$ is requested:

\begin{equation}
\lambda_m = p_m \lambda \, , \text{where} \, \sum_{m=1}^N p_m = 1 \, .
\end{equation}

%We consider a sharing probabilty $p_{share}$ which specifies the share of home routers in the AS that supports the caching functionality and serves as tier-1 cache.

%\begin{itemize}
%	\item Number IP-Adresses -> Number of Routers
%	\item Caches and Sharing Probability
%  	\item Replication Factor -> optimize with http://conferences.sigcomm.org/co-next/2009/papers/Valancius.pdf and http://arxiv.org/pdf/1004.4709.pdf
%  	\item Capacity of an AS, effective cache capacity
%    \item ratio upload download bandwidth -> should ISPs change their contracts?
%  \item locality of requests -> higher potential of the overlay
%\end{itemize}

\begin{table}[tb]
\centering
\caption{Notation of the paper.}
\label{tab:notation}
\begin{tabular}{|c|c|c|}
\hline
param. & description & default \\
\hline
$N$ & catalogue size & 1e6 \\
$n_{1}$ & number of tier-1 caches & 1e4 \\
$n_{2}$ & number of tier-2 caches & 1 \\
$C_{1}$ & tier-1 cache capacity & 8 \\
$C_{2}$ & tier-2 cache capacity & 1e4 \\
$\rho_{1}$ & tier-1 cache upload bandwidth & 0.8Mbps \\
$\lambda_m$ & arrival rate of requests for object $m$& \\
$b_m$ & bit rate of object $m$& \\
$d_m$ & duration of object $m$& \\
%%$\bar{\lambda}$ & average arrival rate of requests& \\
%$r_m$ & number of replications of object m& \\
\hline
\end{tabular}
\end{table}

\subsection{Content Popularity and Traffic Model}\label{sec:hierarchical:background:traffic}

In order to evaluate the performance of content delivery networks, the arrival process of objects needs to be specified.
The standard approach to characterize the pattern of object requests arriving at a cache is the Independent Reference Model (IRM) \cite{coffman1973operating}.
The IRM makes the following assumptions:

\newtheorem{irma}{Assumption}\begin{irma}\label{catalouge}
Users request objects from a catalogue with fixed size $N$.
\end{irma}
\newtheorem{irmb}[irma]{Assumption}\begin{irmb}\label{pmc}
The object popularity does not vary over time, i.e., the probability $p_m$ that an item $m, 1\leq m \leq N$ is requested, is constant.
\end{irmb}
\newtheorem{irmc}[irma]{Assumption}\begin{irmc}\label{iid}
The probability $p_m$ that an item is requested, is independent of all past requests, generating an i.i.d request process.
\end{irmc}

The IRM ignores temporal correlations in the request process.
In practice the request rate of an object increases in a short period of time.
This effect is referred to as \textit{temporal locality} and can have a strong positive impact on the efficiency of caching.

To account for temporal locality the request process can be modeled as renewal process \cite{martina2014unified}.
In the renewal traffic model the request process for every content $m$ is described by an independent process with assigned inter-request time distribution.
In this case the request process for each content is stationary.

Stationary request processes also result in a static content popularity, which is a very strong assumption.
In practice the popularity of contents to be cached can be extremely dynamic over time.
In modern content delivery networks, such as YouTube, a high number of new contents is uploaded every single day.
While some contents are active only a few days after publishing, e.g. news, other contents, such as songs, remain popular over a long period of time.
Hence, the content popularity is highly dynamic and can have a high impact on the efficiency of caching, since the variation of the request rates happens on time scales which are comparable or even smaller than the churn time of caches.

To cover non-stationary macroscopic effects related to dynamic content popularity the Shot Noise Model (SNM) is proposed in \cite{traverso2013temporal}.
It represents the overall request process as the superposition of a potentially infinite population of independent inhomogeneous Poisson processes, which are referred to as shots.
Analytic models for LRU caches under SNM traffic can be developed using the Che approximation.

The analysis of non-LRU caches is very difficult under SNM traffic.
In order to analyze the impact of dynamic content popularity on non-LRU cache, \cite{garetto2014dynamic} propose a traffic model based on a Markov modulated Poisson Process.
The Markov modulated Poisson process describes an ON-OFF process for a given content $m$.
The ON and OFF periods are exponentially distributed.
During an ON period the request rate for an item $\lambda_m$ is constant.
The model allows simple analysis if the OFF period is set much larger than the cache eviction time.
This makes the probability negligible that an item $m$ is still in the cache at the end of its OFF period.
An ON period in the ON-OFF traffic model plays exactly the same role as a (rectangular) shot in the SNM traffic model.
The authors show in \cite{garetto2014dynamic} that the cache efficiency under the SNM traffic model can be predicted with high accuracy by adopting a fixed-size content catalogue, and modeling the arrival process of each content by a renewal process with a specific inter-request time distribution.

The popularity of objects is modeled by a Zipf-like law, which is frequently observed for different types of content distributed in the Internet, including video \cite{gill2007youtube,cha2009analyzing}.
The Zipf law states that the probability to request the object with rank $r$, i.e., the $r$-th most popular object, is proportional to $r^{-\alpha}$.
% \begin{equation}
%   p(m)\sim r(m)^{-\alpha}
% \end{equation}
The exponent $\alpha$ has a high impact on the cache performance and ranges between 0.65 and 1 depending on the system and the type of object \cite{fricker2012impact, zink2009characteristics}.

\subsection{Caching Strategies}\label{sec:hierarchical:background:strategies}

In the following we give a representative set of caching strategies.
The caching strategy decides which object in the cache is evicted if a newly requested item has to be stored in the cache.

\begin{itemize}
  \itemsep0em
  \item RANDOM: The simplest way to choose an item to make room for a new object is by random.
  \item LFU: The Least Frequently Used policy evicts the least frequently used item.
  It stores the most popular items in the cache.
  LFU performs optimal under IRM traffic.
  \item LRU: If a newly requested item is not in the cache, it is stored in the cache. The Least Recently Used item is evicted if the cache is full.
  A well known problem of LRU caches is cache pollution, which occurs if objects are replaced by less frequently requested items or items that are requested only once.
  \item q-LRU: If a newly requested item is not in the cache, it is stored in the cache with probability $q$. The Least Recently Used item is evicted if the cache is full.
  The probability ob being stored in the caches is higher for frequently requested objects, which prevents cache pollution. \cite{martina2014unified}
  \item k-LRU: In the k-LRU policy $k-1$ virtual caches storing only object hashes precede the actual cache $k$. An object is only stored in cache $i$, if it is found in its preceding cache $i-1$. The eviction policy of the caches is LRU.
  The virtual caches function as filters to prevent cache pollution. \cite{martina2014unified}
  \item SG-LRU: Score Gated LRU caching strategies attributes a store to each object. A newly requested object is only stored in the cache if it has a higher score that the bottom object. The score functions can be based on statistics of past requests approaching the LFU policy if the memory is large. \cite{hasslinger2014caching}
  \item LRL: If the capacity of caches is limited to serve requests due to bandwidth or processing constraints, requests for an item are blocked although the item is stored in the cache. The Least Recently Lost strategy evicts the object for which a request was least recently or never blocked. \cite{leconte2014adaptive}
  %\item LBW: \cite{zhou2015unifying}
\end{itemize}

In a system of interconnected caches, such as the hierarchical caching system described in \refsec{sec:hierarchical:system_model}, requests that cannot be served at one cache or in one tier produce a miss and are forwarded to the next tier.
In this way the requests traverse a route towards the repository which stores all objects, until they finally hit the target.
The following replication strategies for cache networks decide how the object is replicated on the route traversed by the request \cite{rossini2014coupling,martina2014unified}:

\begin{itemize}
  \item LCE (leave-copy-everywhere): the object is sent to all caches on the backward path.
  \item LCD (leave-copy-down): the object is sent only to the cache preceding the one in which the object is found.
  \item LCP (leave-copy-probabilistically): the object is sent with probability $q$ to each cache on the backward path.
\end{itemize}

\subsection{Performance Models for Hierarchical Caching Systems}\label{sec:hierarchical:background:models}

A vast amount of studies on performance models for hierarchical caching systems have been conducted recently.
\reftab{tab:litoverview} gives an overview on the literature on performance evaluation of caching systems.
The table shows different categories, considering the caching strategy, the cache topology, the request processes, caching constraints and the methods used for performance evaluation and optimization.
In the following we describe the performance models in more detail.

% \begin{sidewaystable}
%   \centering
%   \caption{Overview on literature on performance evaluation of caching systems TODO update policies / Methods / ....}
%   \label{tab:litoverview}
%   \resizebox{\textwidth}{!}{%
%   \begin{tabular}{|l|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|} \hline
%     \textbf{Study} & \textbf{Policies} & \textbf{Topology} & \textbf{Traffic Model} & \textbf{Constraints} & \textbf{Placement} & \textbf{Method} \\ \hline \hline
%     Che et al. \cite{che2002hierarchical} & LRU & Hierarchical & Static - IRM & Cache capacity & No & Analysis \\ \hline
%     Applegate et al. \cite{applegate2010optimal} & LRU & Hierarchical & Static - IRM & Cache capacity, bandwidth & No & Simulation \\ \hline
%     Rosensweig et al. \cite{rosensweig2010approximate} & LRU & General cache networks & Static - IRM & Cache capacity & No & Analysis \\ \hline
%     Fricker et al. \cite{fricker2012impact,fricker2012versatile} & LRU & Hierarchical & Static - IRM, Zipf, general & Cache capacity & No & Analysis \\ \hline
%     Hasslinger et al. \cite{hasslinger2014caching} & LRU, SG-LRU, SLWND & Single cache & Static - IRM, Zipf & Cache capacity & No & Analysis \\ \hline
%     Martina et al. \cite{martina2014unified} & LRU, q-LRU, k-LRU & General cache networks & Static - IRM, renewal & Cache capacity & No & Analysis \\ \hline
%     Valancius et al. \cite{valancius2009greening} & LRU, HWC & Hierarchical & Trace & Cache capacity, bandwidth & HWC & Trace driven simulation \\ \hline
%     Tan et al. \cite{tan2013optimal} & LRU, HWC & Hierarchical & Static - Zipf & Cache capacity, bandwidth & HWC & Optimization \\ \hline
%     Leconte et al. \cite{leconte2014adaptive} & LRL & Hierarchical & Dynamic - Zipf & Cache capacity, bandwidth & LRL & TODO \\ \hline
%     Zhou et al. \cite{zhou2015unifying} & LBW & Hierarchical & Dynamic - Zipf & Cache capacity, bandwidth & LBW & TODO \\ \hline
%     Garetto et al. \cite{garetto2014dynamic} & LRU & Hierarchical & Dynamic - OnOff & Cache capacity & No & Analysis \\ \hline
% Traverso et al. \cite{traverso2013temporal} & LRU & Single Cache & Dynamic - SNM & Cache capacity & No & Analysis \\ \hline
%     Leconte et al. \cite{leconte2016dynamic} & LRU & Hierarchical & Dynamic - SNM & Cache capacity & No & Analysis \\ \hline
%   \end{tabular}}
% \end{sidewaystable}

\begin{table}
  \definecolor{myGray}{RGB}{221,223,255}
  \centering
  \caption{Overview on literature on performance evaluation of caching systems.}
  \label{tab:litoverview}

%\begin{sidewaystable}
%\begin{sideways}
\begin{turn}{270}

  %\caption{Overview on literature on performance evaluation of caching systems.}
  %\vspace{0.3cm}
	%\label{tab:litoverview}

    %\resizebox{\textwidth}{!}{%
	\begin{tabular}{|l|ccccc|cc|cc|cc|cccc|} \hline
		& \rotatebox{90}{LRU policy} & \rotatebox{90}{q-LRU / k-LRU policy} & \rotatebox{90}{score based policy (SG-LRU)} & \rotatebox{90}{loss / bw based policy (LRL)} & \rotatebox{90}{optimal placement (HWC)} & \rotatebox{90}{cache hierarchy} & \rotatebox{90}{general cache network} & \rotatebox{90}{temporal locality} & \rotatebox{90}{popularity dynamics} & \rotatebox{90}{bandwidth constraints} & \rotatebox{90}{inter-domain traffic} & \rotatebox{90}{trace driven simulation} & \rotatebox{90}{simulation (synthetic traffic)} &\rotatebox{90}{analysis}& \rotatebox{90}{optimization} \\
		\hline
		Che et al. \cite{che2002hierarchical} & $\bullet$ & -- & -- & -- & -- & $\bullet$ & -- & -- & -- & -- & -- & $\bullet$ & -- & $\bullet$ & -- \\
    Applegate et al. \cite{applegate2010optimal} & $\bullet$ & -- & -- & -- & -- & $\bullet$ & -- & -- & -- & $\bullet$ & -- & -- & $\bullet$ & -- & $\bullet$ \\
    Rosensweig et al. \cite{rosensweig2010approximate} & $\bullet$ & -- & -- & -- & -- & $\bullet$ & $\bullet$ & -- & -- & -- & -- & -- & -- & $\bullet$ & -- \\
    Fricker et al. \cite{fricker2012impact,fricker2012versatile} & $\bullet$ & -- & -- & -- & -- & $\bullet$ & -- & $\bullet$ & -- & -- & -- & $\bullet$ & $\bullet$ & $\bullet$ & -- \\
    Hasslinger et al. \cite{hasslinger2014caching} & $\bullet$ & -- & $\bullet$ & -- & -- & -- & -- & -- & -- & -- & -- & -- & $\bullet$ & -- & -- \\
    Martina et al. \cite{martina2014unified} & $\bullet$ & $\bullet$ & -- & -- & -- & $\bullet$ & $\bullet$ & $\bullet$ & -- & -- & -- & -- & $\bullet$ & $\bullet$ & -- \\
    Valancius et al. \cite{valancius2009greening} & $\bullet$ & -- & -- & -- & $\bullet$ & $\bullet$ & -- & -- & -- & $\bullet$ & -- & $\bullet$ & -- & -- & $\bullet$ \\
    Tan et al. \cite{tan2013optimal} & $\bullet$ & -- & -- & -- & $\bullet$ & $\bullet$ & -- & -- & -- & $\bullet$ & -- & -- & -- & $\bullet$ & $\bullet$ \\
    Leconte et al. \cite{leconte2014adaptive} & -- & -- & -- & $\bullet$ & -- & -- & -- & -- & -- & $\bullet$ & -- & -- & $\bullet$ & $\bullet$ & $\bullet$ \\
    Zhou et al. \cite{zhou2015unifying} & -- & -- & -- & $\bullet$ & -- & -- & -- & -- & -- & $\bullet$ & -- & -- & $\bullet$ & $\bullet$ & $\bullet$ \\
    Traverso et al. \cite{traverso2013temporal} & $\bullet$ & -- & -- & -- & -- & -- & -- & $\bullet$ & $\bullet$ & -- & -- & $\bullet$ & $\bullet$ & $\bullet$ & -- \\
    Garetto et al. \cite{garetto2014dynamic} & $\bullet$ & $\bullet$ & -- & -- & -- & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & -- & -- & $\bullet$ & $\bullet$ & $\bullet$ & -- \\
    Leconte et al. \cite{leconte2016dynamic} & $\bullet$ & -- & -- & -- & -- & $\bullet$ & -- & $\bullet$ & $\bullet$ & -- & -- & $\bullet$ & $\bullet$ & $\bullet$ & -- \\ \hline
    Lareida et al. \cite{lareida2015augmenting} & $\bullet$ & -- & -- & -- & -- & $\bullet$ & -- & -- & -- & -- & $\bullet$ & $\bullet$ & $\bullet$ & -- & -- \\
    Burger et al. \cite{burger2016hierarchical} & $\bullet$ & -- & -- & -- & $\bullet$ & $\bullet$ & -- & -- & -- & $\bullet$ & -- & -- & $\bullet$ & $\bullet$ & -- \\
% 		\rowcolor{myGray} Our approach with varying user den\-si\-ties & $\bullet$ & -- & $\bullet$ & \large\textcolor{red}{$\bullet$} & $\bullet$ & -- & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ & \textcolor{red}{$\bullet$} & -- & $\bullet$ & -- & -- & -- & \textcolor{red}{$\bullet$} \\
		\hline
	\end{tabular}%}
\end{turn}
\end{table}

\subsubsection{Analytic Performance Models for Caching Systems}

%In the following we give a brief overview of analytic models for the evaluation of mechanisms in CDN and CCN architectures.
%Requests that cannot be served in the first tier are forwarded to the second tier.
To analyze the performance of isolated and interconnected caches, many works leverage the Che approximation \cite{che2002hierarchical}, which provides a decoupling technique for LRU caches.
The requests are generated according to the IRM, which assumes identically and independently distributed requests of a set of objects.
%In \cite{fricker2012impact} a two tiered caching architecture is investigated, where the first tier consists of home routers in access networks. The second tier consists of data centers in the core network.
It is shown in \cite{fricker2012versatile, fricker2012impact} that the model also applies in more general conditions.
The model also provides accurate results for a high number of objects with varying file sizes.
In \cite{martina2014unified} the model is extended, to analyze advanced caching strategies like {$k$-LRU}, where objects have to pass a certain number of $k-1$ virtual caches to be stored in the actual cache.
The virtual caches replace objects according to LRU and store only meta information.
The IRM assumptions are generalized in order to apply to effects of temporal locality in the request process.
The model for LRU can be further extended to evaluate the performance of general cache networks \cite{rosensweig2010approximate, martina2014unified}.
Further caching strategies with limited memory, like W-LFU and Geometrical Fading, are investigated in \cite{hasslinger2014caching}.
The Sliding Window strategy applies the LFU approach to the frequency of requests in a limited time frame.
Geometrical Fading scores recent requests with a factor that decreases according to a geometric sequences with the number of intermediate request.

\subsubsection{Evaluation of Caching Systems with Dynamic Content Popularity}

Only few studies have investigated the effects of dynamic content popularity.
The renewal traffic model, which generalizes the IRM, allows capturing temporal locality in the traffic \cite{martina2014unified}.
However, the request process generated by the renewal traffic model is still stationary.

To cover non-stationary macroscopic effects related to dynamic content popularity, a Shot Noise Model (SNM) is proposed in \cite{traverso2013temporal}. As already mentioned, for LRU caches analytic models based on the Che approximation can be developed under SNM traffic.
To allow analysis of non-LRU policies the ON-OFF model is proposed in \cite{garetto2014dynamic} using an on-off-modulated Poisson process.
In \cite{leconte2016dynamic} the analysis under SNM traffic is further extended for caches with small population, such as base-stations, home routers or set-top boxes.
%\cite{hasslinger2016itc} wikipedia trace

\subsubsection{Evaluation of Caching Systems with Limited Capacity}

The analytic models described above do not consider the service time of requests, which is limited by the bandwidth of the uplink of the cache.
In this monograph we consider a tiered caching architecture, where the upload bandwidth of the caches is highly limited.
In order to evaluate hierarchical caching systems for different content demand models considering social and temporal dynamics, different caching and resource selection policies, we develop a simulation framework, which is described in detail in \refsec{sec:hierarchical:simulative:simulative}.
%A limited bandwidth or service time of the caches is not considered in any of the above analytic models.
The paper closest to this works is \cite{valancius2009greening}, which proposes the NaDa approach and develops an optimal content placement on NaDas.
The performance of the approach is evaluated with traces only.
%\cite{applegate2010optimal}
To evaluate the performance analytically, considering bandwidth constraints, the system can be modeled as loss system consisting of a server for each of the caches.
The exact stationary distribution of the loss system is too complex to evaluate due to the high number of feasible content placements.
In \cite{tan2013optimal} the system is analyzed under large system asymptotic where the number of NaDas goes to infinity and simplifications occur.

In order to evaluate the system for a finite number of NaDas, we use a different approach by approximating the arrival rate of requests.
This allows us to effectively assess the loss probability by using a simple form of the Erlang formula for a loss system, c.f., \refsec{sec:hierarchical:analyticbw:model}.
%We combine the

To optimize the content placement adaptively, \cite{leconte2014adaptive} propose Least-Recently-Lost (LRL) replacement, which tries to optimize the loss rate in the first tier.
A different approach is proposed by \cite{zhou2015unifying} which allocates bandwidth resources instead of content copies proportionally to the content popularity.
